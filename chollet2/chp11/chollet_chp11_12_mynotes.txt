Skimmed chp 11 and 12  

Conclusion interesting, says don't believe the hype of deep learning.  
It's not understanding or intelligence, but many dimensional manifold, 
addition, subtraction, and %delta.  Better for Perception problems.  

Where humans use logic or reasoning, deep learning is lacking.  
This is OK, as long as we use both approaches.  
Alpha Go started by using known human strategy and moves.  
But later was able to learn winning moves without strategy as long 
as game rules were provided.  Game rules were simple compared to life.  

Transformers - attention is all you need, 2017 paper - downloaded.
Try code chp 11 and read source paper.  

 * rest of chp 11 is 2/3, many pages.  
 * need to run it.  May need AWS  
 * Peter not coming today.  Dan and Me.  Maybe redo Parts 2 and 3 of Chp 11 next time with Peter?  
 
### Chp 11 part 1, TPU  
Github doesn't show any results/outputs from run, even though it's there when viewed on my laptop.  
 https://nbviewer.org/github/JennEYoon/deep-learning/blob/main/chollet2/chollet_chp11_p01_intro.ipynb  
 Outputs shown in nbviewer.  
 
Part 2 Sequence - rerun on Colab, jenneyoon@gmail.com acct, fresh acct GPU.  
Much faster.  Still kind of slow. Takes long time to run 5 epoches, 30 min?  
  ~ 6.5 minutes (475 sec) per epoch.  There's a pause 1 minute? between stop and start of each epoch.  

Part 3 Transformers - run 1 GPU using datasciY.info@gmail.com acct.  
Twice as fast, 20 epochs.  ~1.7 minute (100 sec) per epoch.  


P4 error: calls "layers" before it is imported, first time only.  Fixed no on Chollet Github?  
P3 error: extra ")" in custom function.  Fixed?  
Look for errata on Chollet Github.  If not fixed, submit pull request.  
