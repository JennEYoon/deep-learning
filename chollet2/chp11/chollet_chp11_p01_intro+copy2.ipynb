{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzLYnDlGu5cm"
      },
      "source": [
        "### Jennifer Yoon. 3/28/2022 run 1   \n",
        "Chollet 2nd ed, chp 11 part 1 preprocessing data.  \n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yYRwyUKu5cr"
      },
      "source": [
        "# Deep learning for text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf_Ix4z6u5cs"
      },
      "source": [
        "## Natural-language processing: The bird's eye view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URgB5iQKu5cs"
      },
      "source": [
        "## Preparing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPaQ2Aeou5cu"
      },
      "source": [
        "### Text standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vyj95GHSu5cu"
      },
      "source": [
        "### Text splitting (tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAMA6Ddgu5cv"
      },
      "source": [
        "### Vocabulary indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkpALG04u5cw"
      },
      "source": [
        "### Using the TextVectorization layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YtPdv5f3u5cw"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = self.standardize(text)\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict(\n",
        "            (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string?\n",
        "\"\"\" output\n",
        "Type:        module\n",
        "String form: <module 'string' from '/usr/lib/python3.7/string.py'>\n",
        "File:        /usr/lib/python3.7/string.py\n",
        "Docstring:  \n",
        "A collection of string constants.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3Ne8aN4nwfKE",
        "outputId": "cbf145c9-b245-41b6-abf7-b94fab989b57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" output\\nType:        module\\nString form: <module 'string' from '/usr/lib/python3.7/string.py'>\\nFile:        /usr/lib/python3.7/string.py\\nDocstring:  \\nA collection of string constants.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6M0jlGqbwgFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47-PQjfBu5cz",
        "outputId": "f85fe8a0-a58e-481c-b653-dbbbcd2b0ca4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iATdTAUMu5cz",
        "outputId": "3689898c-40d6-44bc-c7f7-001f02db4efa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fUL03gSu5c0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KRXr3O2u5c0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "    lowercase_string = tf.strings.lower(string_tensor)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "    return tf.strings.split(string_tensor)\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ50A8RZu5c1"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MeditUIu5c2"
      },
      "source": [
        "**Displaying the vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWVo2R_eu5c3",
        "outputId": "826a9dd0-18c5-4ead-a3a5-e2d28f24bd83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "text_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFHOky5Ju5c4",
        "outputId": "9ff70b36-2fec-4526-c863-e0e0685f4a37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsNE9oVLu5c4",
        "outputId": "ab159fe6-0747-4e1a-a688-37dc1222539d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCYEs92ou5c5"
      },
      "source": [
        "## Two approaches for representing groups of words: Sets and sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-upqq1pou5c5"
      },
      "source": [
        "### Preparing the IMDB movie reviews data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZY6smeeu5c6",
        "outputId": "7c853a3f-31cf-406a-9369-33b50b3712d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  21.9M      0  0:00:03  0:00:03 --:--:-- 21.9M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqsoj_0Ru5c7"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q8fYbxGu5c7",
        "outputId": "4fd7ede1-2eca-43ee-88ea-5b7c86bcbf6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ],
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqNA8jdtu5c8"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgY22wZOu5c8",
        "outputId": "651c8fe9-d07d-41e7-f61a-d3d0d9a76b44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwV6XCUKu5c9"
      },
      "source": [
        "**Displaying the shapes and dtypes of the first batch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUbe6hzDu5c9",
        "outputId": "83968036-7d7a-4dde-bad0-c8cd620fe4cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'I disliked Frosty returns and this one. Both of the films are absurd with poorly written characters and bad voices. The voices could have been done but Frosty returns was worst than this because of the unalike snowman they drew. Frosty\\'s winterwonder land was great and so was the original heartwarming Drama. The only good thing about this film was Frosty the snowman was alike of the classic 60s and a funny line like when they were talking about things that are not real and then Frosty walks by the window humming, whistling whatever he was doing but everything else was badly written and badly animated and I barely got the plot. I saw the DVD cover at the library about 2 or 3 years ago and the art looked crappy so I didn\\'t pick it up until this year because the pictures on the back looked a little touching but I played this film. It was a hassle poorly art characters just like the DVD and it was just terrible about what\\'s going on. I watched this at night and found it dull that I fell asleep on the whole thing (Wait I always do that when I watch something while I go to sleep) But this movie was overall boring<br /><br />Overall rating: Grade -C<br /><br />This is like another holiday special bad rip-off in the frosty returns and this one. This one is the badly 4th sequel like Home alone 4 well i thought that movie was OK but home alone 3 was great. But it\\'s like a bad home alone 3 and 4 people hate like this. My sister didn\\'t watch it that much because when she came into my room to watch 2 minutes and a half of it, she walked away like \"I hate the animation!\" I agree to her just like the 3rd one Frosty returns. Plain awful!', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S6JOGaYu5c9"
      },
      "source": [
        "### Processing words as a set: The bag-of-words approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDv6wQGgu5c-"
      },
      "source": [
        "#### Single words (unigrams) with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhnOcEiKu5c-"
      },
      "source": [
        "**Preprocessing our datasets with a `TextVectorization` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaZTq8Exu5c-"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX4V8Ewbu5c_"
      },
      "source": [
        "**Inspecting the output of our binary unigram dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeZL4KE0u5c_",
        "outputId": "4d677262-c2e5-41a3-a41a-9ef8399cc8b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[0]\n"
      ],
      "metadata": {
        "id": "yr0dxlGVxxkc",
        "outputId": "3d4de1da-590e-4829-8cea-f97dbe8fb7dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(20000,), dtype=float32, numpy=array([1., 1., 1., ..., 0., 0., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets[0]"
      ],
      "metadata": {
        "id": "zu_uzP6Yxxq0",
        "outputId": "46f6ebd4-c9ee-4cc1-807d-ac5edc92d63a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIsCrwdvu5c_"
      },
      "source": [
        "**Our model-building utility**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re9ipteNu5c_"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uliu3k6Au5dA"
      },
      "source": [
        "**Training and testing the binary unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjFfl-4Iu5dA",
        "outputId": "59475c5a-031a-491c-a9a4-378ed4ffabe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 14s 21ms/step - loss: 0.4038 - accuracy: 0.8338 - val_loss: 0.2723 - val_accuracy: 0.8926\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2688 - accuracy: 0.9016 - val_loss: 0.2677 - val_accuracy: 0.8958\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2426 - accuracy: 0.9126 - val_loss: 0.2795 - val_accuracy: 0.8982\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2238 - accuracy: 0.9215 - val_loss: 0.2953 - val_accuracy: 0.8980\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2193 - accuracy: 0.9299 - val_loss: 0.3067 - val_accuracy: 0.8992\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2119 - accuracy: 0.9315 - val_loss: 0.3141 - val_accuracy: 0.8980\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2063 - accuracy: 0.9343 - val_loss: 0.3233 - val_accuracy: 0.8968\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2085 - accuracy: 0.9360 - val_loss: 0.3315 - val_accuracy: 0.8968\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1981 - accuracy: 0.9374 - val_loss: 0.3413 - val_accuracy: 0.8964\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1975 - accuracy: 0.9390 - val_loss: 0.3502 - val_accuracy: 0.8964\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 0.2892 - accuracy: 0.8868\n",
            "Test acc: 0.887\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVn-0cKVu5dD"
      },
      "source": [
        "#### Bigrams with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3C4ALFtu5dE"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return bigrams**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8T1SrUy9u5dE"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x607WXAu5dE"
      },
      "source": [
        "**Training and testing the binary bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENnXTlIeu5dE",
        "outputId": "89aa3a44-fbbd-49d5-9338-f9736460a894",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.3733 - accuracy: 0.8446 - val_loss: 0.2630 - val_accuracy: 0.8974\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2383 - accuracy: 0.9141 - val_loss: 0.2848 - val_accuracy: 0.8968\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2028 - accuracy: 0.9331 - val_loss: 0.2858 - val_accuracy: 0.9032\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1868 - accuracy: 0.9426 - val_loss: 0.3076 - val_accuracy: 0.9012\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1835 - accuracy: 0.9459 - val_loss: 0.3194 - val_accuracy: 0.9030\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1755 - accuracy: 0.9503 - val_loss: 0.3361 - val_accuracy: 0.9020\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.1808 - accuracy: 0.9524 - val_loss: 0.3446 - val_accuracy: 0.9016\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1678 - accuracy: 0.9538 - val_loss: 0.3521 - val_accuracy: 0.8988\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1753 - accuracy: 0.9549 - val_loss: 0.3526 - val_accuracy: 0.8992\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1734 - accuracy: 0.9546 - val_loss: 0.3609 - val_accuracy: 0.8972\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 0.2673 - accuracy: 0.8954\n",
            "Test acc: 0.895\n"
          ]
        }
      ],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ABwTTnLe0U9U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC1V6hrlu5dE"
      },
      "source": [
        "#### Bigrams with TF-IDF encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_J-ha64u5dF"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return token counts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omEe0EoTu5dF"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeY3JD-Ku5dF"
      },
      "source": [
        "**Configuring `TextVectorization` to return TF-IDF-weighted outputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHWbChtGu5dF"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlgsBMQhu5dF"
      },
      "source": [
        "**Training and testing the TF-IDF bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbAMtk-Yu5dF",
        "outputId": "39b9a633-82fb-4b78-98d0-cbd0661a57c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.5304 - accuracy: 0.7568 - val_loss: 0.3603 - val_accuracy: 0.8756\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.3923 - accuracy: 0.8397 - val_loss: 0.3188 - val_accuracy: 0.8788\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.3649 - accuracy: 0.8500 - val_loss: 0.3777 - val_accuracy: 0.8668\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.3415 - accuracy: 0.8595 - val_loss: 0.3469 - val_accuracy: 0.8768\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.3265 - accuracy: 0.8649 - val_loss: 0.3469 - val_accuracy: 0.8822\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.3156 - accuracy: 0.8732 - val_loss: 0.3951 - val_accuracy: 0.8772\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2973 - accuracy: 0.8803 - val_loss: 0.3645 - val_accuracy: 0.8726\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2919 - accuracy: 0.8860 - val_loss: 0.3742 - val_accuracy: 0.8714\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2799 - accuracy: 0.8878 - val_loss: 0.4314 - val_accuracy: 0.8752\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2736 - accuracy: 0.8916 - val_loss: 0.4263 - val_accuracy: 0.8760\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 0.3143 - accuracy: 0.8788\n",
            "Test acc: 0.879\n"
          ]
        }
      ],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_eFGz6Yu5dG"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SviU70jPu5dG",
        "outputId": "0e9fb09f-4071-4e25-87eb-40085714aa1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78.53 percent positive\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes:  \n",
        "\n",
        "Thus far, not very accurate.  \n",
        "Colab very fast today. zipped through all the training.  \n",
        "Pre-processing to review later, code.  Try code from scratch.  "
      ],
      "metadata": {
        "id": "K_14w2-u0Yxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jXvL1zOO0Y_Q"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "chollet_chp11_p01_intro.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}