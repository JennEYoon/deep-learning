{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0wibqR3b1a1"
      },
      "source": [
        "# Chollet chp11 p4, seq-to-seq, Colab GPU run 2.  \n",
        "Author: Jennifer E Yoon  \n",
        "Date: April 2, 2022 11pm   \n",
        "Run 2 on Colab, initial work through all without stopping and experimenting.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### My Note:  \n",
        "Date: June 12, 2022  \n",
        "This one has end-to-end Transformer encoder and decoder example, using English to Spanish translations.  \n",
        "Would be easier if I know a bit of Spanish. French and English examples are easier since I know a bit of French.  \n",
        "Try this with French?  Korean?  \n",
        "How is the positional embedding mask used?  \n",
        "Need to carefully step through code in this notebook.  \n",
        "Look at saved model outputs, saved to outside of repo.  \n",
        "Jennifer Yoon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJqr6qa1b1a6"
      },
      "source": [
        "## Beyond text classification: Sequence-to-sequence learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYDn5c7xb1a7"
      },
      "source": [
        "### A machine translation example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfSGXTgzb1a7",
        "outputId": "88d78090-ad42-4a73-cfeb-45065f8cf78e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-04-03 03:25:11--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 108.177.97.128, 108.177.125.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "\rspa-eng.zip           0%[                    ]       0  --.-KB/s               \rspa-eng.zip         100%[===================>]   2.52M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-04-03 03:25:11 (106 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aqJNlyJzb1a9"
      },
      "outputs": [],
      "source": [
        "text_file = \"spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, spanish = line.split(\"\\t\")\n",
        "    spanish = \"[start] \" + spanish + \" [end]\"\n",
        "    text_pairs.append((english, spanish))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_muWWCywdgQF",
        "outputId": "1196526b-f291-47fc-a66a-783fe6c5a243"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Everyone was listening very carefully.',\n",
              "  '[start] Todos estaban escuchando atentamente. [end]'),\n",
              " ('Tom said that he wanted to learn French.',\n",
              "  '[start] Tom dijo que quería aprender francés. [end]'),\n",
              " ('My cigar went out. Will you give me a light?',\n",
              "  '[start] Se me apagó el cigarro. ¿Quiere usted darme lumbre? [end]'),\n",
              " ('He gets tired easily.', '[start] Se cansa con facilidad. [end]'),\n",
              " (\"I don't believe he is a lawyer.\", '[start] Creo que no es abogado. [end]'),\n",
              " ('I thought you and Tom were friends.',\n",
              "  '[start] Pensé que Tom y vos eran amigos. [end]'),\n",
              " ('Tom is a fighter.', '[start] Tom es un luchador. [end]'),\n",
              " ('Everyone in the class learned the poem by heart.',\n",
              "  '[start] Todos en la clase aprendieron el poema de memoria. [end]'),\n",
              " ('How much did the glasses cost?',\n",
              "  '[start] ¿Cuánto costaron los lentes? [end]'),\n",
              " ('The money was all there. Nobody touched it.',\n",
              "  '[start] Estaba íntegro el dinero, nadie lo tocó. [end]')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_pairs[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTOcV1Uyb1a9",
        "outputId": "2876ca8e-d148-431c-b8b4-8e352ac11506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('I saw Tom on Monday.', '[start] Tom y yo nos vimos el lunes. [end]')\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cvsuBVawb1a-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uRpQw4oeHVR",
        "outputId": "e6791792-3d7f-469b-809e-d1a1733bc657"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(\"I still can't dance.\", '[start] Sigo sin saber bailar. [end]'),\n",
              " ('We count on you.', '[start] Contamos con ustedes. [end]'),\n",
              " ('Do you speak Swahili?', '[start] ¿Hablas suajili? [end]')]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_pairs[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5Ve1Hd7eWjT",
        "outputId": "8ffa6b87-fe6d-4149-ebc0-59714e07a4ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('The celebrations culminated in a spectacular fireworks display.',\n",
              "  '[start] El festival terminó con una espectacular exhibición de fuegos artificiales. [end]'),\n",
              " (\"I've made a list of things I'd like to buy.\",\n",
              "  '[start] He hecho una lista de las cosas que me gustaría comprar. [end]'),\n",
              " ('Never tell a lie!', '[start] ¡No mientas nunca! [end]')]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_pairs[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGdyRd_ieWrK",
        "outputId": "77b490a0-d50f-4285-ee41-082de32d57ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Everyone was listening very carefully.',\n",
              "  '[start] Todos estaban escuchando atentamente. [end]'),\n",
              " ('Tom said that he wanted to learn French.',\n",
              "  '[start] Tom dijo que quería aprender francés. [end]'),\n",
              " ('My cigar went out. Will you give me a light?',\n",
              "  '[start] Se me apagó el cigarro. ¿Quiere usted darme lumbre? [end]')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_pairs[0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaYJIdICb1a_"
      },
      "source": [
        "**Vectorizing the English and Spanish text pairs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Hm5MSuOqb1bA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras  # edit, add import here\n",
        "from tensorflow.keras import layers  # edit, add import here, error: \"layers not defined\".\n",
        "\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VGB2U9glf0Mo",
        "outputId": "7eb1ea2f-d93a-49d4-e0c4-a6ebb79d1aff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Everyone was listening very carefully.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_english_texts[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0RvudTK3f0Zg",
        "outputId": "2e64ad9c-ee81-4dc4-d08c-8d464855b177"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[start] Todos estaban escuchando atentamente. [end]'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_spanish_texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IA1R8oWegob8"
      },
      "outputs": [],
      "source": [
        "target_vectorization?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNkaLoOwb1bB"
      },
      "source": [
        "**Preparing datasets for the translation task**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dyxnA87xb1bC"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = source_vectorization(eng)\n",
        "    spa = target_vectorization(spa)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"spanish\": spa[:, :-1],\n",
        "    }, spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHtAeYyxb1bC",
        "outputId": "ce8c3e4c-d741-4089-dbf0-3d3c27c44bd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgVfimRZb1bD"
      },
      "source": [
        "### Sequence-to-sequence learning with RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JirKHzf3b1bD"
      },
      "source": [
        "**GRU-based encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iTRIyTgBb1bE"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = layers.Bidirectional(\n",
        "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40TXFKMRb1bE"
      },
      "source": [
        "**GRU-based decoder and the end-to-end model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Rv9TzrmMb1bF"
      },
      "outputs": [],
      "source": [
        "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIJi_YKWb1bF"
      },
      "source": [
        "**Training our recurrent sequence-to-sequence model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bo9Hu_obb1bG",
        "outputId": "541f3ed9-bf50-49f9-add0-85b0b0594511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1302/1302 [==============================] - 232s 164ms/step - loss: 1.6327 - accuracy: 0.4176 - val_loss: 1.3229 - val_accuracy: 0.5029\n",
            "Epoch 2/15\n",
            "1302/1302 [==============================] - 208s 160ms/step - loss: 1.3141 - accuracy: 0.5254 - val_loss: 1.1593 - val_accuracy: 0.5665\n",
            "Epoch 3/15\n",
            "1302/1302 [==============================] - 208s 160ms/step - loss: 1.1709 - accuracy: 0.5764 - val_loss: 1.0743 - val_accuracy: 0.6003\n",
            "Epoch 4/15\n",
            "1302/1302 [==============================] - 207s 159ms/step - loss: 1.0794 - accuracy: 0.6083 - val_loss: 1.0365 - val_accuracy: 0.6192\n",
            "Epoch 5/15\n",
            "1302/1302 [==============================] - 207s 159ms/step - loss: 1.0342 - accuracy: 0.6324 - val_loss: 1.0261 - val_accuracy: 0.6288\n",
            "Epoch 6/15\n",
            "1302/1302 [==============================] - 207s 159ms/step - loss: 1.0045 - accuracy: 0.6506 - val_loss: 1.0208 - val_accuracy: 0.6346\n",
            "Epoch 7/15\n",
            "1302/1302 [==============================] - 207s 159ms/step - loss: 0.9854 - accuracy: 0.6639 - val_loss: 1.0214 - val_accuracy: 0.6378\n",
            "Epoch 8/15\n",
            "1302/1302 [==============================] - 207s 159ms/step - loss: 0.9716 - accuracy: 0.6739 - val_loss: 1.0238 - val_accuracy: 0.6409\n",
            "Epoch 9/15\n",
            "1302/1302 [==============================] - 207s 159ms/step - loss: 0.9615 - accuracy: 0.6829 - val_loss: 1.0253 - val_accuracy: 0.6411\n",
            "Epoch 10/15\n",
            "1302/1302 [==============================] - 207s 159ms/step - loss: 0.9537 - accuracy: 0.6883 - val_loss: 1.0280 - val_accuracy: 0.6439\n",
            "Epoch 11/15\n",
            "1302/1302 [==============================] - 208s 159ms/step - loss: 0.9483 - accuracy: 0.6925 - val_loss: 1.0290 - val_accuracy: 0.6444\n",
            "Epoch 12/15\n",
            "1302/1302 [==============================] - 207s 159ms/step - loss: 0.9441 - accuracy: 0.6960 - val_loss: 1.0336 - val_accuracy: 0.6442\n",
            "Epoch 13/15\n",
            "1302/1302 [==============================] - 208s 160ms/step - loss: 0.9425 - accuracy: 0.6975 - val_loss: 1.0359 - val_accuracy: 0.6436\n",
            "Epoch 14/15\n",
            "1302/1302 [==============================] - 208s 160ms/step - loss: 0.9409 - accuracy: 0.6992 - val_loss: 1.0391 - val_accuracy: 0.6436\n",
            "Epoch 15/15\n",
            "1302/1302 [==============================] - 209s 160ms/step - loss: 0.9407 - accuracy: 0.6996 - val_loss: 1.0410 - val_accuracy: 0.6436\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f61f0409290>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq2seq_rnn.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17MF7nqRb1bG"
      },
      "source": [
        "**Translating new sentences with our RNN encoder and decoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6KzZf4Jb1bH",
        "outputId": "015cc620-13d3-4a30-d3d8-08d5b834f955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-\n",
            "There are lots of things to do.\n",
            "[start] hay muchas cosas que hacer [end]\n",
            "-\n",
            "I sometimes feel hungry in the middle of the night.\n",
            "[start] a veces me [UNK] en hambre de la noche [end]\n",
            "-\n",
            "I am thinking about that matter.\n",
            "[start] estoy pensando en eso [end]\n",
            "-\n",
            "I have a car.\n",
            "[start] tengo un coche [end]\n",
            "-\n",
            "He got up to see if he had turned off the light in the kitchen.\n",
            "[start] Él se [UNK] a ver si la [UNK] se lo en la televisión [end]\n",
            "-\n",
            "It was a dangerous time.\n",
            "[start] fue un momento [end]\n",
            "-\n",
            "When I was your age, I was already married.\n",
            "[start] cuando era tu edad ya estaba [end]\n",
            "-\n",
            "What's the secret to success?\n",
            "[start] cuál es el secreto de su éxito [end]\n",
            "-\n",
            "He took a day off.\n",
            "[start] Él se tomó un día [end]\n",
            "-\n",
            "We're surrounded.\n",
            "[start] estamos de casa [end]\n",
            "-\n",
            "Don't ask me why but, he ran away when he saw me.\n",
            "[start] no me [UNK] por qué me dijo cuando él me había dado [end]\n",
            "-\n",
            "I don't remember anyone named Tom.\n",
            "[start] no recuerdo a tom nadie [end]\n",
            "-\n",
            "I missed my bus this morning.\n",
            "[start] perdí mi viaje por esta mañana [end]\n",
            "-\n",
            "The conference ended at five.\n",
            "[start] la lluvia el cinco años [end]\n",
            "-\n",
            "\"Where's his book?\" \"It's on the table.\"\n",
            "[start] dónde está su libro está sobre la mesa [end]\n",
            "-\n",
            "He was the leader of the great expedition.\n",
            "[start] Él fue el nombre del una del tom [end]\n",
            "-\n",
            "You're the only person I know in Boston.\n",
            "[start] eres la persona que conozco a boston [end]\n",
            "-\n",
            "Where's today's newspaper?\n",
            "[start] dónde está el dinero de hoy [end]\n",
            "-\n",
            "I hate it when that happens.\n",
            "[start] odio cuando eso está en [end]\n",
            "-\n",
            "I couldn't answer all of the questions.\n",
            "[start] no pude todas las preguntas [end]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "        next_token_predictions = seq2seq_rnn.predict(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyPxSj3gb1bH"
      },
      "source": [
        "### Sequence-to-sequence learning with Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahf7qyfwb1bI"
      },
      "source": [
        "#### The Transformer decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHApTw9Kb1bI"
      },
      "source": [
        "**The `TransformerDecoder`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UK7Btkr9b1bI"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqPTRMFg6ti2"
      },
      "source": [
        "**Transformer encoder** implemented as a subclassed Layer  \n",
        "Copied from Chapter 11 Part3 Transformer notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "57caFog_6X8y"
      },
      "outputs": [],
      "source": [
        "# TransformerEncoder class definition  \n",
        "#import tensorflow as tf\n",
        "#from tensorflow import keras\n",
        "#from tensorflow.keras import layers\n",
        "\n",
        "# Copied from p3 Transformers notebook \n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-WOQp-D6YAY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG1LqQVpb1bJ"
      },
      "source": [
        "#### Putting it all together: A Transformer for machine translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qQ62bdDb1bJ"
      },
      "source": [
        "**PositionalEmbedding layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KFcU-Lg-b1bJ"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA8t07S-b1bK"
      },
      "source": [
        "**End-to-end Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "YkebIn94b1bK"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "# TransformerEncoder() not defined.  \"De\"coder? No.\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs0pxfJMb1bL"
      },
      "source": [
        "**Training the sequence-to-sequence Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLB9tX1Ob1bL",
        "outputId": "fc1f22b6-d984-439e-c1ca-f16582c7b369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1302/1302 [==============================] - 169s 126ms/step - loss: 1.7392 - accuracy: 0.3942 - val_loss: 1.4181 - val_accuracy: 0.4735\n",
            "Epoch 2/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 1.4225 - accuracy: 0.5011 - val_loss: 1.2279 - val_accuracy: 0.5451\n",
            "Epoch 3/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 1.2508 - accuracy: 0.5573 - val_loss: 1.1306 - val_accuracy: 0.5820\n",
            "Epoch 4/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 1.1398 - accuracy: 0.5961 - val_loss: 1.0652 - val_accuracy: 0.6118\n",
            "Epoch 5/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 1.0827 - accuracy: 0.6217 - val_loss: 1.0356 - val_accuracy: 0.6289\n",
            "Epoch 6/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 1.0480 - accuracy: 0.6398 - val_loss: 1.0231 - val_accuracy: 0.6354\n",
            "Epoch 7/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 1.0222 - accuracy: 0.6541 - val_loss: 1.0220 - val_accuracy: 0.6361\n",
            "Epoch 8/30\n",
            "1302/1302 [==============================] - 165s 126ms/step - loss: 1.0013 - accuracy: 0.6654 - val_loss: 1.0146 - val_accuracy: 0.6411\n",
            "Epoch 9/30\n",
            "1302/1302 [==============================] - 165s 126ms/step - loss: 0.9831 - accuracy: 0.6752 - val_loss: 1.0037 - val_accuracy: 0.6492\n",
            "Epoch 10/30\n",
            "1302/1302 [==============================] - 166s 127ms/step - loss: 0.9672 - accuracy: 0.6838 - val_loss: 1.0088 - val_accuracy: 0.6488\n",
            "Epoch 11/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.9524 - accuracy: 0.6911 - val_loss: 1.0066 - val_accuracy: 0.6516\n",
            "Epoch 12/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.9403 - accuracy: 0.6974 - val_loss: 1.0133 - val_accuracy: 0.6501\n",
            "Epoch 13/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.9265 - accuracy: 0.7036 - val_loss: 1.0099 - val_accuracy: 0.6558\n",
            "Epoch 14/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.9141 - accuracy: 0.7092 - val_loss: 1.0129 - val_accuracy: 0.6546\n",
            "Epoch 15/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.9010 - accuracy: 0.7143 - val_loss: 1.0169 - val_accuracy: 0.6548\n",
            "Epoch 16/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.8921 - accuracy: 0.7183 - val_loss: 1.0172 - val_accuracy: 0.6572\n",
            "Epoch 17/30\n",
            "1302/1302 [==============================] - 163s 125ms/step - loss: 0.8803 - accuracy: 0.7233 - val_loss: 1.0233 - val_accuracy: 0.6551\n",
            "Epoch 18/30\n",
            "1302/1302 [==============================] - 163s 126ms/step - loss: 0.8697 - accuracy: 0.7277 - val_loss: 1.0283 - val_accuracy: 0.6548\n",
            "Epoch 19/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.8578 - accuracy: 0.7313 - val_loss: 1.0308 - val_accuracy: 0.6590\n",
            "Epoch 20/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.8515 - accuracy: 0.7347 - val_loss: 1.0416 - val_accuracy: 0.6567\n",
            "Epoch 21/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.8416 - accuracy: 0.7391 - val_loss: 1.0375 - val_accuracy: 0.6561\n",
            "Epoch 22/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.8322 - accuracy: 0.7422 - val_loss: 1.0397 - val_accuracy: 0.6573\n",
            "Epoch 23/30\n",
            "1302/1302 [==============================] - 163s 126ms/step - loss: 0.8221 - accuracy: 0.7455 - val_loss: 1.0473 - val_accuracy: 0.6554\n",
            "Epoch 24/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.8140 - accuracy: 0.7485 - val_loss: 1.0551 - val_accuracy: 0.6588\n",
            "Epoch 25/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.8084 - accuracy: 0.7515 - val_loss: 1.0609 - val_accuracy: 0.6565\n",
            "Epoch 26/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.8021 - accuracy: 0.7535 - val_loss: 1.0714 - val_accuracy: 0.6563\n",
            "Epoch 27/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.7929 - accuracy: 0.7569 - val_loss: 1.0869 - val_accuracy: 0.6566\n",
            "Epoch 28/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.7852 - accuracy: 0.7593 - val_loss: 1.0763 - val_accuracy: 0.6570\n",
            "Epoch 29/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.7767 - accuracy: 0.7618 - val_loss: 1.0707 - val_accuracy: 0.6598\n",
            "Epoch 30/30\n",
            "1302/1302 [==============================] - 164s 126ms/step - loss: 0.7734 - accuracy: 0.7633 - val_loss: 1.0908 - val_accuracy: 0.6552\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f617b754550>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxI2BhxZb1bL"
      },
      "source": [
        "**Translating new sentences with our Transformer model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mptW7tsJb1bL",
        "outputId": "9c88ad63-063b-4f1d-a62d-c8efa213ff87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-\n",
            "I found this restaurant by chance.\n",
            "[start] encontré este restaurante de oportunidad [end]\n",
            "-\n",
            "Do whatever makes you happy.\n",
            "[start] haz lo que te haga feliz [end]\n",
            "-\n",
            "I don't know my neighbors.\n",
            "[start] no sé mis a los países [end]\n",
            "-\n",
            "I need to know what happened to Tom.\n",
            "[start] necesito saber qué le pasó a tom [end]\n",
            "-\n",
            "There are some misprints, but all in all, it's a good book.\n",
            "[start] hay algunos [UNK] pero todas [end]\n",
            "-\n",
            "Do you believe war will start?\n",
            "[start] crees que la guerra de [UNK] [end]\n",
            "-\n",
            "We're still friends.\n",
            "[start] todavía somos amigos amigos [end]\n",
            "-\n",
            "You are tired, aren't you?\n",
            "[start] estás cansado verdad [end]\n",
            "-\n",
            "I came as soon as I could.\n",
            "[start] hice tanto como pude [end]\n",
            "-\n",
            "I sleep in my room.\n",
            "[start] yo voy de la habitación a en mi habitación [end]\n",
            "-\n",
            "The cottage reminded me of the happy times I had spent with her.\n",
            "[start] la casa me [UNK] por los veces que había dado con él había dado [end]\n",
            "-\n",
            "A tsunami is coming, so please be on the alert.\n",
            "[start] un [UNK] está en el lugar por favor de estar en el la la la la la la [UNK] [end]\n",
            "-\n",
            "Ten years is a really long period of time.\n",
            "[start] diez años es un largo tiempo tú realmente [UNK] [end]\n",
            "-\n",
            "Did you listen to music last night?\n",
            "[start] has escrito música anoche [end]\n",
            "-\n",
            "Soccer is an exciting game.\n",
            "[start] el fútbol es un juego interesante [end]\n",
            "-\n",
            "I repeated exactly what he had said.\n",
            "[start] yo yo [UNK] qué dijo [end]\n",
            "-\n",
            "Please turn on the radio.\n",
            "[start] [UNK] la radio por favor [end]\n",
            "-\n",
            "Do you have a bicycle?\n",
            "[start] tiene una bicicleta [end]\n",
            "-\n",
            "You do a great job.\n",
            "[start] hiciste un gran trabajo [end]\n",
            "-\n",
            "Life's short.\n",
            "[start] las vida [UNK] [end]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwXpdkaFb1bM"
      },
      "source": [
        "## Summary"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "chollet_chp11_p4_seq-to-seq_run2end",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
